{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98387576",
   "metadata": {},
   "source": [
    "# Extract and encode MIMIC data\n",
    "We extract the mimic information and encode it as time-series data. \n",
    "We use the following tables: \n",
    "- labevents: \n",
    "    - contains blood tests, etc. \n",
    "- microbiology events: \n",
    "\n",
    "- output events: \n",
    "    - contains \n",
    "    \n",
    "- chartevents\n",
    "    - contains vital signs\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3961be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94becd9b",
   "metadata": {},
   "source": [
    "Define the relative or absolute data path. The mimic data must be in the folders: \n",
    "- data_root + '/mimiciv/3.0/hosp/' \n",
    "- data_root + '/mimiciv/3.0/icu/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e8d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a relative or absolute path to the mimic folder. \n",
    "# data_path = '/../data/real_world_data/physionet.org_small/files'\n",
    "# data_path = '/../data/real_world_data/physionet.org/files'\n",
    "data_path = '/../data/real_world_data/physionet.org_small/files'\n",
    "# data_path = \"/lustre/groups/labs/marr/qscd01/datasets/vonKleist/physionet.org/files\"\n",
    "root_dir = os.path.dirname(os.path.abspath('UserInterface.ipynb')) + data_path\n",
    "\n",
    "# define path for processed files\n",
    "target_path = './data/features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d0c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "module_path='preprocessing/day_intervals_preproc'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "module_path='utils'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "module_path='preprocessing/hosp_module_preproc'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "module_path='model'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "#print(sys.path)\n",
    "# define the path to the data from current repository \n",
    "# (from here data is in mimiciv/3.0/hosp )\n",
    "root_dir = os.path.dirname(os.path.abspath('UserInterface.ipynb')) + data_path\n",
    "\n",
    "# root_dir = data_path\n",
    "\n",
    "import day_intervals_cohort\n",
    "from day_intervals_cohort import *\n",
    "\n",
    "import day_intervals_cohort_v2\n",
    "from day_intervals_cohort_v2 import *\n",
    "\n",
    "import day_intervals_cohort_v3\n",
    "from day_intervals_cohort_v3 import *\n",
    "\n",
    "import data_generation_icu\n",
    "\n",
    "import data_generation\n",
    "import evaluation\n",
    "\n",
    "import feature_selection_hosp\n",
    "from feature_selection_hosp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "available-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is only for training\n",
    "import ml_models\n",
    "from ml_models import *\n",
    "\n",
    "import dl_train\n",
    "from dl_train import *\n",
    "\n",
    "import tokenization\n",
    "from tokenization import *\n",
    "\n",
    "import behrt_train\n",
    "from behrt_train import *\n",
    "\n",
    "import feature_selection_icu\n",
    "from feature_selection_icu import *\n",
    "import fairness\n",
    "import callibrate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nutritional-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(day_intervals_cohort)\n",
    "import day_intervals_cohort\n",
    "from day_intervals_cohort import *\n",
    "\n",
    "importlib.reload(day_intervals_cohort_v2)\n",
    "import day_intervals_cohort_v2\n",
    "from day_intervals_cohort_v2 import *\n",
    "\n",
    "importlib.reload(day_intervals_cohort_v3)\n",
    "import day_intervals_cohort_v3\n",
    "from day_intervals_cohort_v3 import *\n",
    "\n",
    "\n",
    "importlib.reload(data_generation_icu)\n",
    "import data_generation_icu\n",
    "importlib.reload(data_generation)\n",
    "import data_generation\n",
    "\n",
    "importlib.reload(feature_selection_hosp)\n",
    "import feature_selection_hosp\n",
    "from feature_selection_hosp import *\n",
    "\n",
    "importlib.reload(feature_selection_icu)\n",
    "import feature_selection_icu\n",
    "from feature_selection_icu import *\n",
    "\n",
    "importlib.reload(tokenization)\n",
    "import tokenization\n",
    "from tokenization import *\n",
    "\n",
    "importlib.reload(ml_models)\n",
    "import ml_models\n",
    "from ml_models import *\n",
    "\n",
    "importlib.reload(dl_train)\n",
    "import dl_train\n",
    "from dl_train import *\n",
    "\n",
    "importlib.reload(behrt_train)\n",
    "import behrt_train\n",
    "from behrt_train import *\n",
    "\n",
    "importlib.reload(fairness)\n",
    "import fairness\n",
    "\n",
    "importlib.reload(callibrate_output)\n",
    "import callibrate_output\n",
    "\n",
    "importlib.reload(evaluation)\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-demand",
   "metadata": {},
   "source": [
    "# Welcome to your MIMIC-IV Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-crown",
   "metadata": {},
   "source": [
    "This repository explains the steps to download and clean MIMIC-IV dataset for analysis.\n",
    "The repository is compatible with MIMIC-IV v1.0 and MIMIC-IV v2.0\n",
    "\n",
    "Please go to:\n",
    "- https://physionet.org/content/mimiciv/1.0/ for v1.0\n",
    "- https://physionet.org/content/mimiciv/2.0/ for v2.0\n",
    "- https://physionet.org/content/mimiciv/3.0/ for v3.0\n",
    "\n",
    "Follow instructions to get access to MIMIC-IV dataset.\n",
    "\n",
    "Download the files using your terminal: \n",
    "- wget -r -N -c -np --user mehakg --ask-password https://physionet.org/files/mimiciv/1.0/ or\n",
    "- wget -r -N -c -np --user mehakg --ask-password https://physionet.org/files/mimiciv/2.0/ or\n",
    "- wget -r -N -c -np --user mehakg --ask-password https://physionet.org/files/mimiciv/3.0/        \n",
    "\n",
    "Save downloaded files in the parent directory of this github repo. \n",
    "\n",
    "The structure should look like below for v1.0-\n",
    "- mimiciv/1.0/core\n",
    "- mimiciv/1.0/hosp\n",
    "- mimiciv/1.0/icu\n",
    "\n",
    "The structure should look like below for v2.0-\n",
    "- mimiciv/2.0/hosp\n",
    "- mimiciv/2.0/icu\n",
    "\n",
    "The structure should look like below for v3.0-\n",
    "- mimiciv/3.0/hosp\n",
    "- mimiciv/3.0/icu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-tissue",
   "metadata": {},
   "source": [
    "## 1. DATA EXTRACTION\n",
    "Please run below cell to select option for cohort selection.\n",
    "The cohort will be svaed in **./data/cohort/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "structured-dimension",
   "metadata": {
    "tags": [
     "{",
     "\"tags\":",
     "[",
     "\"hide-input\"",
     "]",
     "}"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select the approriate version of MIMIC-IV for which you have downloaded data ?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1d94dc7ca145a182bee3bcfc9aa8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(index=2, options=('Version 1', 'Version 2', 'Version 3'), value='Version 3')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select what prediction task you want to perform ?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a63901304542da943fc1b776db0c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(index=3, options=('Mortality', 'Length of Stay', 'Readmission', 'Phenotype'), value='Phenotype')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Please select the approriate version of MIMIC-IV for which you have downloaded data ?\")\n",
    "version = widgets.RadioButtons(options=['Version 1','Version 2','Version 3'],value='Version 3')\n",
    "display(version)\n",
    "\n",
    "print(\"Please select what prediction task you want to perform ?\")\n",
    "radio_input4 = widgets.RadioButtons(options=['Mortality','Length of Stay','Readmission','Phenotype'],value='Phenotype')\n",
    "display(radio_input4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-syndicate",
   "metadata": {},
   "source": [
    "### Refining Cohort and Prediction Task Definition\n",
    "\n",
    "Based on your current selection following block will provide option to further refine prediction task and cohort associated with it:\n",
    "\n",
    "- First you will refine the prediction task choosing from following options -\n",
    "    - **Length of Stay** - You can select from two predefined options or enter custom number of days to predict length os stay greater than number of days.\n",
    "\n",
    "    - **Readmission** - You can select from two predefined options or enter custom number of days to predict readmission after \"number of days\" after previous admission.\n",
    "\n",
    "    - **Phenotype Prediction** - You can select from four major chronic diseases to predict its future outcome\n",
    "\n",
    "        - Heart failure\n",
    "        - CAD (Coronary Artery Disease)\n",
    "        - CKD (Chronic Kidney Disease)\n",
    "        - COPD (Chronic obstructive pulmonary disease)\n",
    "\n",
    "- Second, you will choode whether to perfom above task using ICU or non-ICU admissions data\n",
    "\n",
    "- Third, you can refine the refine the cohort selection for any of the above choosen prediction tasks by including the admission samples admitted with particular chronic disease - \n",
    "    - Heart failure\n",
    "    - CAD (Coronary Artery Disease)\n",
    "    - CKD (Chronic Kidney Disease)\n",
    "    - COPD (Chronic obstructive pulmonary disease)\n",
    "    \n",
    "print(\"**Please run below cell to extract the cohort for selected options**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broke-spirituality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify which prediction task to perform:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359dd72f4e504e13958062565f56b949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(options=('No specific prediction', 'Heart Failure in 30 days', 'CAD in 30 days', 'CKD in 30 days'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Data\n",
      "Please select below if you want to work with ICU or Non-ICU data ?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d1361dea084c0fafecd0ce0bd82324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(options=('ICU', 'Non-ICU'), value='ICU')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select if you want to perform choosen prediction task for a specific disease.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43e01f5bf154c61913bc4077ae19f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(options=('No Disease Filter', 'Heart Failure', 'CKD', 'CAD', 'COPD'), value='No Disease Filter')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define what data to load \n",
    "print(\"Please specify which prediction task to perform:\")\n",
    "if radio_input4.value=='Length of Stay':\n",
    "    options=['Length of Stay ge 3','Length of Stay ge 7','Custom']\n",
    "    value='Length of Stay ge 3'\n",
    "    radio_input2 = widgets.RadioButtons(options = option, value=  value)\n",
    "    display(radio_input2)\n",
    "    text1=widgets.IntSlider(\n",
    "    value=3,\n",
    "    min=1,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "    display(widgets.HBox([widgets.Label('Length of stay ge (in days)',layout={'width': '180px'}), text1]))\n",
    "elif radio_input4.value=='Readmission':\n",
    "    options=['30 Day Readmission','60 Day Readmission','90 Day Readmission',\n",
    "             '120 Day Readmission','Custom']\n",
    "    value= '30 Day Readmission'\n",
    "    radio_input2 = widgets.RadioButtons(options = options,value = value)\n",
    "    display(radio_input2)\n",
    "    text1=widgets.IntSlider(\n",
    "    value=30,\n",
    "    min=10,\n",
    "    max=150,\n",
    "    step=10,\n",
    "    disabled=False\n",
    "    )\n",
    "    display(widgets.HBox([widgets.Label('Readmission after (in days)',layout={'width': '180px'}), text1]))\n",
    "elif radio_input4.value=='Phenotype':\n",
    "    options = ['No specific prediction', 'Heart Failure in 30 days','CAD in 30 days','CKD in 30 days','COPD in 30 days']\n",
    "    value ='No specific prediction'\n",
    "    radio_input2 = widgets.RadioButtons(options=options, value = value)\n",
    "    display(radio_input2)\n",
    "elif radio_input4.value=='Mortality':\n",
    "    options=['Mortality']\n",
    "    value='Mortality'\n",
    "    radio_input2 = widgets.RadioButtons(options=options,value=value)\n",
    "    #display(radio_input2)\n",
    "\n",
    "print(\"Extract Data\")\n",
    "print(\"Please select below if you want to work with ICU or Non-ICU data ?\")\n",
    "radio_input1 = widgets.RadioButtons(options=['ICU', 'Non-ICU'],value='ICU')\n",
    "display(radio_input1)\n",
    "\n",
    "print(\"Please select if you want to perform choosen prediction task for a specific disease.\")\n",
    "radio_input3 = widgets.RadioButtons(options=['No Disease Filter','Heart Failure','CKD','CAD','COPD'],value='No Disease Filter')\n",
    "display(radio_input3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "republican-freight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========MIMIC-IV v3.0============\n",
      "EXTRACTING FOR: | ICU |  DUE TO NO_LABEL | 100000 | \n",
      "[ COHORT SUCCESSFULLY SAVED ]\n",
      "[ SUMMARY SUCCESSFULLY SAVED ]\n",
      "ICU DATA\n",
      "# Admission Records: 2999\n",
      "# Patients: 1960\n"
     ]
    }
   ],
   "source": [
    "disease_label=\"\"\n",
    "time=0\n",
    "label=radio_input4.value\n",
    "\n",
    "if label=='Readmission':\n",
    "    if radio_input2.value=='Custom':\n",
    "        time=text1.value\n",
    "    else:\n",
    "        time=int(radio_input2.value.split()[0])\n",
    "elif label=='Length of Stay':\n",
    "    if radio_input2.value=='Custom':\n",
    "        time=text1.value\n",
    "    else:\n",
    "        time=int(radio_input2.value.split()[4])\n",
    "\n",
    "if label=='Phenotype':    \n",
    "    if radio_input2.value=='Heart Failure in 30 days':\n",
    "        label='Readmission'\n",
    "        time=30\n",
    "        disease_label='I50'\n",
    "    elif radio_input2.value=='CAD in 30 days':\n",
    "        label='Readmission'\n",
    "        time=30\n",
    "        disease_label='I25'\n",
    "    elif radio_input2.value=='CKD in 30 days':\n",
    "        label='Readmission'\n",
    "        time=30\n",
    "        disease_label='N18'\n",
    "    elif radio_input2.value=='COPD in 30 days':\n",
    "        label='Readmission'\n",
    "        time=30\n",
    "        disease_label='J44'\n",
    "    elif radio_input2.value=='No specific prediction':\n",
    "        label=''\n",
    "        time=100000\n",
    "        disease_label='no_label'\n",
    "    \n",
    "data_icu=radio_input1.value==\"ICU\"\n",
    "data_mort=label==\"Mortality\"\n",
    "data_admn=label=='Readmission'\n",
    "data_los=label=='Length of Stay'\n",
    "        \n",
    "\n",
    "if (radio_input3.value==\"Heart Failure\"):\n",
    "    icd_code='I50'\n",
    "elif (radio_input3.value==\"CKD\"):\n",
    "    icd_code='N18'\n",
    "elif (radio_input3.value==\"COPD\"):\n",
    "    icd_code='J44'\n",
    "elif (radio_input3.value==\"CAD\"):\n",
    "    icd_code='I25'\n",
    "else:\n",
    "    icd_code='No Disease Filter'\n",
    "\n",
    "if version.value=='Version 1':\n",
    "    version_path= \"mimiciv/1.0\"\n",
    "    cohort_output = day_intervals_cohort.extract_data(radio_input1.value,label,time,icd_code, root_dir,disease_label)\n",
    "elif version.value=='Version 2':\n",
    "    version_path= \"mimiciv/2.0\"\n",
    "    cohort_output = day_intervals_cohort_v2.extract_data(radio_input1.value,label,time,icd_code, root_dir,disease_label)\n",
    "elif version.value=='Version 3':\n",
    "    version_path= \"mimiciv/3.0\"\n",
    "    cohort_output = day_intervals_cohort_v3.extract_data(radio_input1.value,label,time,icd_code, root_dir,disease_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-stadium",
   "metadata": {},
   "source": [
    "## 2. FEATURE SELECTION\n",
    "Features available for ICU data -\n",
    "- Diagnosis (https://mimic.mit.edu/docs/iv/modules/hosp/diagnoses_icd/)\n",
    "- Procedures (https://mimic.mit.edu/docs/iv/modules/icu/procedureevents/)\n",
    "- Medications (https://mimic.mit.edu/docs/iv/modules/icu/inputevents/)\n",
    "- Output Events (https://mimic.mit.edu/docs/iv/modules/icu/outputevents/)\n",
    "- Chart Events (https://mimic.mit.edu/docs/iv/modules/icu/chartevents/)\n",
    "- Lab Events (https://mimic.mit.edu/docs/iv/modules/hosp/labevents/)\n",
    "- Microbiology Events (https://mimic.mit.edu/docs/iv/modules/hosp/microbiologyevents/)\n",
    "\n",
    "All features will be saved in the defined **target_path**\n",
    "\n",
    "**Please run below cell to select features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "raised-olympus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Selection\n",
      "Which Features you want to include for cohort?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a59bfe440041edbd73b10046f4b1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Diagnosis')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33dfcef3cd6d4ae791656a7bc2ddb84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Output Events')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6df051495b40f78d5bfaf26b946a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Chart Events')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d9b62af99444ee8c5ee03a2c350e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Procedures')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb62f8622074c1988a5f79a66426da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Medications')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f413aa387be844b0bd8ca5a2f78d745e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Lab Events')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d5fdb4f3654710974f8bf3d94a5a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Microbiology Events')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icu\n",
      "**Please run below cell to extract selected features**\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Selection\")\n",
    "if data_icu:\n",
    "    print(\"Which Features you want to include for cohort?\")\n",
    "    check_input1 = widgets.Checkbox(description='Diagnosis', value=True)\n",
    "    display(check_input1)\n",
    "    check_input2 = widgets.Checkbox(description='Output Events', value=True)\n",
    "    display(check_input2)\n",
    "    check_input3 = widgets.Checkbox(description='Chart Events', value=True)\n",
    "    display(check_input3)\n",
    "    check_input4 = widgets.Checkbox(description='Procedures', value=True)\n",
    "    display(check_input4)\n",
    "    check_input5 = widgets.Checkbox(description='Medications', value=True)\n",
    "    display(check_input5)\n",
    "    check_input100 = widgets.Checkbox(description='Lab Events', value=True)\n",
    "    display(check_input100)\n",
    "    check_input101 = widgets.Checkbox(description='Microbiology Events', value=False)\n",
    "    display(check_input101)\n",
    "\n",
    "    print(\"icu\")\n",
    "else:\n",
    "    print(\"Which Features you want to include for cohort?\")\n",
    "    check_input1 = widgets.Checkbox(description='Diagnosis', value=True)\n",
    "    display(check_input1)\n",
    "    check_input2 = widgets.Checkbox(description='Labs', value=True)\n",
    "    display(check_input2)\n",
    "    check_input3 = widgets.Checkbox(description='Procedures', value=True)\n",
    "    display(check_input3)\n",
    "    check_input4 = widgets.Checkbox(description='Medications', value=True )\n",
    "    display(check_input4)\n",
    "print(\"**Please run below cell to extract selected features**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "420826ee-6187-4bba-8638-c58e4efc0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_flag=check_input1.value\n",
    "out_flag=check_input2.value\n",
    "chart_flag=check_input3.value\n",
    "proc_flag=check_input4.value\n",
    "med_flag=check_input5.value\n",
    "lab_flag=check_input100.value\n",
    "micro_flag=check_input101.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05836bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXTRACTING DIAGNOSIS DATA]\n",
      "[RESULTS] Admissions with diagnoses not in cohort: 4746\n",
      "[RESULTS] Admissions in cohort without diagnosis: 0\n",
      "[RESULTS] Patients with diagnoses not in cohort: 0\n",
      "[RESULTS] Patients in cohort without diagnosis: 0\n",
      "# unique ICD-9 codes 2226\n",
      "# unique ICD-10 codes 3256\n",
      "# unique ICD-10 codes (After converting ICD-9 to ICD-10) 3569\n",
      "# unique ICD-10 codes (After clinical gruping ICD-10 codes) 991\n",
      "# Admissions:   2999\n",
      "Total rows 56933\n",
      "Columns kept for diagnosis: ['subject_id', 'hadm_id', 'stay_id', 'icd_code', 'root_icd10_convert', 'root']\n",
      "[SUCCESSFULLY SAVED DIAGNOSIS DATA]\n",
      "[EXTRACTING OUTPUT EVENTS DATA]\n",
      "[RESULTS] Stays with output info not in cohort: 0\n",
      "[RESULTS] Stays in cohort without output info: 0\n",
      "# Unique Events:   67\n",
      "# Admissions:   2909\n",
      "Total rows 164170\n",
      "Columns kept for output events: ['subject_id', 'hadm_id', 'stay_id', 'itemid', 'charttime', 'intime', 'event_time_from_admit', 'value']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESSFULLY SAVED OUTPUT EVENTS DATA]\n",
      "[EXTRACTING CHART EVENTS DATA]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:24, 12.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows processed: 13383573\n",
      "Total rows with missing 'valuenum': 8180197\n",
      "[RESULTS] Stays in charts not in cohort: 0\n",
      "[RESULTS] Stays in cohort without chart info: 0\n",
      "# Unique Events:   2027\n",
      "# Admissions:   2999\n",
      "Total rows 11730995\n",
      "Columns kept for chart events: ['stay_id', 'itemid', 'event_time_from_admit', 'valuenum']\n",
      "[SUCCESSFULLY SAVED CHART EVENTS DATA]\n",
      "[EXTRACTING PROCEDURES DATA]\n",
      "[RESULTS] Stays with proc not in cohort: 0\n",
      "[RESULTS] Stays in cohort without proc info: 319\n",
      "# Unique Events:   143\n",
      "# Admissions:   2680\n",
      "Total rows 24050\n",
      "Columns kept for procedures: ['subject_id', 'hadm_id', 'stay_id', 'itemid', 'starttime', 'intime', 'event_time_from_admit']\n",
      "[SUCCESSFULLY SAVED PROCEDURES DATA]\n",
      "[EXTRACTING MEDICATION DATA]\n",
      "[RESULTS] Stays with medication data not in cohort: 0\n",
      "[RESULTS] Stays in cohort without medications: 313\n",
      "# of unique type of drug:  139\n",
      "# Admissions:   2546\n",
      "# Total rows 173465\n",
      "Columns kept for medication: ['subject_id', 'hadm_id', 'stay_id', 'itemid', 'starttime', 'endtime', 'start_hours_from_admit', 'stop_hours_from_admit', 'rate', 'amount', 'orderid']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESSFULLY SAVED MEDICATION DATA]\n",
      "[EXTRACTING LAB EVENTS DATA]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:07,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULTS] Admissions with lab info not in cohort: 4192\n",
      "[RESULTS] Admissions in cohort without lab info: 22\n",
      "# of unique type of lab events:  749\n",
      "# Admissions:   2977\n",
      "# Total rows 1734157\n",
      "Columns kept for lab events: ['subject_id', 'hadm_id', 'stay_id', 'itemid', 'charttime', 'storetime', 'valuenum', 'valueuom', 'ref_range_lower', 'ref_range_upper', 'chart_hours_from_admit', 'store_hours_from_admit']\n",
      "[SUCCESSFULLY SAVED LAB EVENTS DATA]\n"
     ]
    }
   ],
   "source": [
    "# load and preprocess the data (by dropping some columns)\n",
    "if data_icu:\n",
    "    diag_flag=check_input1.value\n",
    "    out_flag=check_input2.value\n",
    "    chart_flag=check_input3.value\n",
    "    proc_flag=check_input4.value\n",
    "    med_flag=check_input5.value\n",
    "\n",
    "    lab_flag=check_input100.value\n",
    "    micro_flag=check_input101.value\n",
    "\n",
    "    \n",
    "    #feature_icu(cohort_output, root_dir, root_dir + '/'+ version_path,diag_flag,out_flag,chart_flag,proc_flag,med_flag)\n",
    "    data = feature_icu( cohort_output = cohort_output, \n",
    "                        root_dir = root_dir, \n",
    "                        version_path = root_dir + '/'+ version_path,\n",
    "                        save_path = target_path, \n",
    "                        diag_flag = diag_flag,\n",
    "                        out_flag = out_flag,\n",
    "                        chart_flag = chart_flag,\n",
    "                        proc_flag = proc_flag,\n",
    "                        med_flag  = med_flag,\n",
    "                        lab_flag = lab_flag,\n",
    "                        micro_flag = micro_flag)\n",
    "else:\n",
    "    # not adapted yet\n",
    "    print(\"Warning: this code hasn't been checked!\")\n",
    "    diag_flag=check_input1.value\n",
    "    lab_flag=check_input2.value\n",
    "    proc_flag=check_input3.value\n",
    "    med_flag=check_input4.value\n",
    "    feature_nonicu(cohort_outputroot_dir, root_dir+ '/'+ version_path , version_path,diag_flag,lab_flag,proc_flag,med_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbdfb2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>itemid</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>valuenum</th>\n",
       "      <th>valueuom</th>\n",
       "      <th>ref_range_lower</th>\n",
       "      <th>ref_range_upper</th>\n",
       "      <th>chart_hours_from_admit</th>\n",
       "      <th>store_hours_from_admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10005593</td>\n",
       "      <td>26835370.0</td>\n",
       "      <td>32896438</td>\n",
       "      <td>53185</td>\n",
       "      <td>2125-06-23 20:18:00</td>\n",
       "      <td>2125-06-24 06:33:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3 days +09:04:58</td>\n",
       "      <td>-3 days +19:19:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10005593</td>\n",
       "      <td>26835370.0</td>\n",
       "      <td>34389119</td>\n",
       "      <td>53185</td>\n",
       "      <td>2125-06-23 20:18:00</td>\n",
       "      <td>2125-06-24 06:33:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3 days +01:56:37</td>\n",
       "      <td>-3 days +12:11:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10005593</td>\n",
       "      <td>26835370.0</td>\n",
       "      <td>32896438</td>\n",
       "      <td>51463</td>\n",
       "      <td>2125-06-23 20:19:00</td>\n",
       "      <td>2125-06-23 22:57:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/hpf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3 days +09:05:58</td>\n",
       "      <td>-3 days +11:43:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10005593</td>\n",
       "      <td>26835370.0</td>\n",
       "      <td>34389119</td>\n",
       "      <td>51463</td>\n",
       "      <td>2125-06-23 20:19:00</td>\n",
       "      <td>2125-06-23 22:57:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/hpf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3 days +01:57:37</td>\n",
       "      <td>-3 days +04:35:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005593</td>\n",
       "      <td>26835370.0</td>\n",
       "      <td>32896438</td>\n",
       "      <td>51464</td>\n",
       "      <td>2125-06-23 20:19:00</td>\n",
       "      <td>2125-06-23 22:57:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3 days +09:05:58</td>\n",
       "      <td>-3 days +11:43:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734152</th>\n",
       "      <td>19997367</td>\n",
       "      <td>21508795.0</td>\n",
       "      <td>36980198</td>\n",
       "      <td>51506</td>\n",
       "      <td>2127-04-16 17:21:00</td>\n",
       "      <td>2127-04-16 18:22:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14 days 04:34:04</td>\n",
       "      <td>14 days 05:35:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734153</th>\n",
       "      <td>19997367</td>\n",
       "      <td>21508795.0</td>\n",
       "      <td>36980198</td>\n",
       "      <td>51508</td>\n",
       "      <td>2127-04-16 17:21:00</td>\n",
       "      <td>2127-04-16 18:22:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14 days 04:34:04</td>\n",
       "      <td>14 days 05:35:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734154</th>\n",
       "      <td>19997367</td>\n",
       "      <td>21508795.0</td>\n",
       "      <td>36980198</td>\n",
       "      <td>51514</td>\n",
       "      <td>2127-04-16 17:21:00</td>\n",
       "      <td>2127-04-16 18:22:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mg/dL</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14 days 04:34:04</td>\n",
       "      <td>14 days 05:35:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734155</th>\n",
       "      <td>19997367</td>\n",
       "      <td>21508795.0</td>\n",
       "      <td>36980198</td>\n",
       "      <td>51516</td>\n",
       "      <td>2127-04-16 17:21:00</td>\n",
       "      <td>2127-04-16 18:22:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>#/hpf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14 days 04:34:04</td>\n",
       "      <td>14 days 05:35:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734156</th>\n",
       "      <td>19997367</td>\n",
       "      <td>21508795.0</td>\n",
       "      <td>36980198</td>\n",
       "      <td>51519</td>\n",
       "      <td>2127-04-16 17:21:00</td>\n",
       "      <td>2127-04-16 18:22:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14 days 04:34:04</td>\n",
       "      <td>14 days 05:35:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1734157 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         subject_id     hadm_id   stay_id  itemid           charttime  \\\n",
       "0          10005593  26835370.0  32896438   53185 2125-06-23 20:18:00   \n",
       "1          10005593  26835370.0  34389119   53185 2125-06-23 20:18:00   \n",
       "2          10005593  26835370.0  32896438   51463 2125-06-23 20:19:00   \n",
       "3          10005593  26835370.0  34389119   51463 2125-06-23 20:19:00   \n",
       "4          10005593  26835370.0  32896438   51464 2125-06-23 20:19:00   \n",
       "...             ...         ...       ...     ...                 ...   \n",
       "1734152    19997367  21508795.0  36980198   51506 2127-04-16 17:21:00   \n",
       "1734153    19997367  21508795.0  36980198   51508 2127-04-16 17:21:00   \n",
       "1734154    19997367  21508795.0  36980198   51514 2127-04-16 17:21:00   \n",
       "1734155    19997367  21508795.0  36980198   51516 2127-04-16 17:21:00   \n",
       "1734156    19997367  21508795.0  36980198   51519 2127-04-16 17:21:00   \n",
       "\n",
       "                  storetime  valuenum valueuom  ref_range_lower  \\\n",
       "0       2125-06-24 06:33:00       NaN      NaN              NaN   \n",
       "1       2125-06-24 06:33:00       NaN      NaN              NaN   \n",
       "2       2125-06-23 22:57:00       NaN     /hpf              NaN   \n",
       "3       2125-06-23 22:57:00       NaN     /hpf              NaN   \n",
       "4       2125-06-23 22:57:00       NaN      NaN              NaN   \n",
       "...                     ...       ...      ...              ...   \n",
       "1734152 2127-04-16 18:22:00       NaN      NaN              NaN   \n",
       "1734153 2127-04-16 18:22:00       NaN      NaN              NaN   \n",
       "1734154 2127-04-16 18:22:00       NaN    mg/dL              0.2   \n",
       "1734155 2127-04-16 18:22:00       4.0    #/hpf              0.0   \n",
       "1734156 2127-04-16 18:22:00       NaN      NaN              NaN   \n",
       "\n",
       "         ref_range_upper chart_hours_from_admit store_hours_from_admit  \n",
       "0                    NaN      -3 days +09:04:58      -3 days +19:19:58  \n",
       "1                    NaN      -3 days +01:56:37      -3 days +12:11:37  \n",
       "2                    NaN      -3 days +09:05:58      -3 days +11:43:58  \n",
       "3                    NaN      -3 days +01:57:37      -3 days +04:35:37  \n",
       "4                    NaN      -3 days +09:05:58      -3 days +11:43:58  \n",
       "...                  ...                    ...                    ...  \n",
       "1734152              NaN       14 days 04:34:04       14 days 05:35:04  \n",
       "1734153              NaN       14 days 04:34:04       14 days 05:35:04  \n",
       "1734154              1.0       14 days 04:34:04       14 days 05:35:04  \n",
       "1734155              5.0       14 days 04:34:04       14 days 05:35:04  \n",
       "1734156              NaN       14 days 04:34:04       14 days 05:35:04  \n",
       "\n",
       "[1734157 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the data \n",
    "data['lab']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-upset",
   "metadata": {},
   "source": [
    "## 3. CLINICAL GROUPING\n",
    "Below you will have option to clinically group diagnosis and medications.\n",
    "Grouping medical codes will reduce dimensional space of features.\n",
    "\n",
    "Default options selected below will group medical codes to reduce feature dimension space.\n",
    "\n",
    "**Please run below cell to select preprocessing for diferent features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "partial-manhattan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to group ICD 10 DIAG codes ?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10c8152c5ec415491258840670d934f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(index=2, layout=Layout(width='100%'), options=('Keep both ICD-9 and ICD-10 codes', 'Convert ICD-9…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Please run below cell to perform feature preprocessing**\n"
     ]
    }
   ],
   "source": [
    "if data_icu:\n",
    "    if diag_flag:\n",
    "        print(\"Do you want to group ICD 10 DIAG codes ?\")\n",
    "        radio_input4 = widgets.RadioButtons(options=['Keep both ICD-9 and ICD-10 codes','Convert ICD-9 to ICD-10 codes','Convert ICD-9 to ICD-10 and group ICD-10 codes'],value='Convert ICD-9 to ICD-10 and group ICD-10 codes',layout={'width': '100%'})\n",
    "        display(radio_input4)   \n",
    "    \n",
    "else:\n",
    "    if diag_flag:\n",
    "        print(\"Do you want to group ICD 10 DIAG codes ?\")\n",
    "        radio_input4 = widgets.RadioButtons(options=['Keep both ICD-9 and ICD-10 codes','Convert ICD-9 to ICD-10 codes','Convert ICD-9 to ICD-10 and group ICD-10 codes'],value='Convert ICD-9 to ICD-10 and group ICD-10 codes',layout={'width': '100%'})\n",
    "        display(radio_input4)     \n",
    "    if med_flag:\n",
    "        print(\"Do you want to group Medication codes to use Non propietary names?\")\n",
    "        radio_input5 = widgets.RadioButtons(options=['Yes','No'],value='Yes',layout={'width': '100%'})\n",
    "        display(radio_input5)\n",
    "    if proc_flag:\n",
    "        print(\"Which ICD codes for Procedures you want to keep in data?\")\n",
    "        radio_input6 = widgets.RadioButtons(options=['ICD-9 and ICD-10','ICD-10'],value='ICD-10',layout={'width': '100%'})\n",
    "        display(radio_input6)\n",
    "print(\"**Please run below cell to perform feature preprocessing**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "descending-symphony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESSING DIAGNOSIS DATA]\n",
      "Total number of rows 54612\n",
      "[SUCCESSFULLY SAVED DIAGNOSIS DATA]\n"
     ]
    }
   ],
   "source": [
    "group_diag=False\n",
    "group_med=False\n",
    "group_proc=False\n",
    "if data_icu:\n",
    "    if diag_flag:\n",
    "        group_diag=radio_input4.value\n",
    "    preprocess_features_icu(cohort_output = cohort_output, \n",
    "                            save_path = target_path, \n",
    "                            diag_flag = diag_flag, \n",
    "                            group_diag = group_diag,\n",
    "                            chart_flag = False, \n",
    "                            clean_chart = False, \n",
    "                            impute_outlier_chart = False,  \n",
    "                            thresh = 0, \n",
    "                            left_thresh= 0)\n",
    "else:\n",
    "    # not adapted yet\n",
    "    print(\"Warning: this code hasn't been checked!\")\n",
    "    if diag_flag:\n",
    "        group_diag=radio_input4.value\n",
    "    if med_flag:\n",
    "        group_med=radio_input5.value\n",
    "    if proc_flag:\n",
    "        group_proc=radio_input6.value\n",
    "    preprocess_features_hosp(cohort_output, diag_flag,proc_flag,med_flag,False,group_diag,group_med,group_proc,False,False,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-captain",
   "metadata": {},
   "source": [
    "### 4. SUMMARY OF FEATURES\n",
    "\n",
    "This step will generate summary of all features extracted so far.<br>\n",
    "It will save summary files in **./data/summary/**<br>\n",
    "- These files provide summary about **mean frequency** of medical codes per admission.<br>\n",
    "- It also provides **total occurrence count** of each medical code.<br>\n",
    "- For labs and chart events it will also provide <br>**missing %** which tells how many rows for a certain medical code has missing value.\n",
    "\n",
    "Please use this information to further refine your cohort by selecting <br>which medical codes in each feature you want to keep and <br>which codes you would like to remove for downstream analysis tasks.\n",
    "\n",
    "**Please run below cell to generate summary files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "thick-residence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GENERATING FEATURE SUMMARY]\n",
      "[SUCCESSFULLY SAVED FEATURE SUMMARY]\n"
     ]
    }
   ],
   "source": [
    "if data_icu:\n",
    "    #generate_summary_icu(diag_flag,proc_flag,med_flag,out_flag,chart_flag)\n",
    "    generate_summary_icu(diag_flag,proc_flag,med_flag,out_flag,chart_flag, lab_flag, micro_flag)\n",
    "else:\n",
    "    generate_summary_hosp(diag_flag,proc_flag,med_flag,lab_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-architecture",
   "metadata": {},
   "source": [
    "## 5. Feature Selection\n",
    "\n",
    "based on the files generated in previous step and other infromation gathered by you,<br>\n",
    "Please select which medical codes you want to include in this study.\n",
    "\n",
    "Please run below cell to to select options for which features you want to perform feature selection.\n",
    "\n",
    "- Select **Yes** if you want to select a subset of medical codes for that feature and<br> **edit** the corresponding feature file for it.\n",
    "- Select **No** if you want to keep all the codes in a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "immediate-seafood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to do Feature Selection for Diagnosis \n",
      " (If yes, please edit list of codes in ./data/summary/diag_features.csv)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2f43ce0ffe494f9c1150d59e589987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(index=1, options=('Yes', 'No'), value='No')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to do Feature Selection for Medication \n",
      " (If yes, please edit list of codes in ./data/summary/med_features.csv)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffed3a59182415c81b17245a2d88f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(index=1, options=('Yes', 'No'), value='No')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to do Feature Selection for Procedures \n",
      " (If yes, please edit list of codes in ./data/summary/proc_features.csv)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2efbd586e9b49bba2ca9045969b6338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(index=1, options=('Yes', 'No'), value='No')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to do Feature Selection for Output event \n",
      " (If yes, please edit list of codes in ./data/summary/out_features.csv)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2e3175a8e344009368c04298c92009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(index=1, options=('Yes', 'No'), value='No')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to do Feature Selection for Chart events \n",
      " (If yes, please edit list of codes in ./data/summary/chart_features.csv)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f8e88646f14424b3accf689e3ea06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(index=1, options=('Yes', 'No'), value='No')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Please run below cell to perform feature selection**\n"
     ]
    }
   ],
   "source": [
    "if data_icu:\n",
    "    if diag_flag:\n",
    "        print(\"Do you want to do Feature Selection for Diagnosis \\n (If yes, please edit list of codes in ./data/summary/diag_features.csv)\")\n",
    "        radio_input4 = widgets.RadioButtons(options=['Yes','No'],value='No')\n",
    "        display(radio_input4)       \n",
    "    if med_flag:\n",
    "        print(\"Do you want to do Feature Selection for Medication \\n (If yes, please edit list of codes in ./data/summary/med_features.csv)\")\n",
    "        radio_input5 = widgets.RadioButtons(options=['Yes','No'],value='No')\n",
    "        display(radio_input5)   \n",
    "    if proc_flag:\n",
    "        print(\"Do you want to do Feature Selection for Procedures \\n (If yes, please edit list of codes in ./data/summary/proc_features.csv)\")\n",
    "        radio_input6 = widgets.RadioButtons(options=['Yes','No'],value='No')\n",
    "        display(radio_input6)   \n",
    "    if out_flag:\n",
    "        print(\"Do you want to do Feature Selection for Output event \\n (If yes, please edit list of codes in ./data/summary/out_features.csv)\")\n",
    "        radio_input7 = widgets.RadioButtons(options=['Yes','No'],value='No')\n",
    "        display(radio_input7)  \n",
    "    if chart_flag:\n",
    "        print(\"Do you want to do Feature Selection for Chart events \\n (If yes, please edit list of codes in ./data/summary/chart_features.csv)\")\n",
    "        radio_input8 = widgets.RadioButtons(options=['Yes','No'],value='No')\n",
    "        display(radio_input8)  \n",
    "else:\n",
    "    if diag_flag:\n",
    "        print(\"Do you want to do Feature Selection for Diagnosis \\n (If yes, please edit list of codes in ./data/summary/diag_features.csv)\")\n",
    "        radio_input4 = widgets.RadioButtons(options=['Yes','No'],value='No')\n",
    "        display(radio_input4)         \n",
    "    if med_flag:\n",
    "        print(\"Do you want to do Feature Selection for Medication \\n (If yes, please edit list of codes in ./data/summary/med_features.csv)\")\n",
    "        radio_input5 = widgets.RadioButtons(options=['Yes','No'],value='No')\n",
    "        display(radio_input5)   \n",
    "    if proc_flag:\n",
    "        print(\"Do you want to do Feature Selection for Procedures \\n (If yes, please edit list of codes in ./data/summary/proc_features.csv)\")\n",
    "        radio_input6 = widgets.RadioButtons(options=['Yes','No'],value='No')\n",
    "        display(radio_input6)   \n",
    "    if lab_flag:\n",
    "        print(\"Do you want to do Feature Selection for Labs \\n (If yes, please edit list of codes in ./data/summary/lab_features.csv)\")\n",
    "        radio_input7 = widgets.RadioButtons(options=['Yes','No'],value='No')\n",
    "        display(radio_input7)   \n",
    "print(\"**Please run below cell to perform feature selection**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "perceived-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_diag=False\n",
    "select_med=False\n",
    "select_proc=False\n",
    "select_lab=False\n",
    "select_out=False\n",
    "select_chart=False\n",
    "\n",
    "if data_icu:\n",
    "    if diag_flag:\n",
    "        select_diag=radio_input4.value == 'Yes'\n",
    "    if med_flag:\n",
    "        select_med=radio_input5.value == 'Yes'\n",
    "    if proc_flag:\n",
    "        select_proc=radio_input6.value == 'Yes'\n",
    "    if out_flag:\n",
    "        select_out=radio_input7.value == 'Yes'\n",
    "    if chart_flag:\n",
    "        select_chart=radio_input8.value == 'Yes'\n",
    "    features_selection_icu(cohort_output, diag_flag,proc_flag,med_flag,out_flag, chart_flag,select_diag,select_med,select_proc,select_out,select_chart)\n",
    "else:\n",
    "    if diag_flag:\n",
    "        select_diag=radio_input4.value == 'Yes'\n",
    "    if med_flag:\n",
    "        select_med=radio_input5.value == 'Yes'\n",
    "    if proc_flag:\n",
    "        select_proc=radio_input6.value == 'Yes'\n",
    "    if lab_flag:\n",
    "        select_lab=radio_input7.value == 'Yes'\n",
    "    features_selection_hosp(cohort_output, diag_flag,proc_flag,med_flag,lab_flag,select_diag,select_med,select_proc,select_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-director",
   "metadata": {},
   "source": [
    "## 6. CLEANING OF FEATURES\n",
    "Below you will have option to to clean lab and chart events by performing outlier removal and unit conversion.\n",
    "\n",
    "Outlier removal is performed to remove values higher than selected **right threshold** percentile and lower than selected **left threshold** percentile among all values for each itemid. \n",
    "\n",
    "**Please run below cell to select preprocessing for diferent features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "moderate-forum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier removal in values of chart events ?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a481ec3b3544e9af574ab9f009b38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(layout=Layout(height='40px', width='100%'), options=('No outlier detection', 'Impute Outlier (def…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbc701cae1b4bb290ed62eadb798cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Right Outlier Threshold', layout=Layout(width='150px')), IntSlider(value=98, layou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a302adf67be745d6b0a75e94c3afad15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Left Outlier Threshold', layout=Layout(width='150px')), IntSlider(value=0, layout=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Please run below cell to perform feature preprocessing**\n"
     ]
    }
   ],
   "source": [
    "if data_icu:\n",
    "    if chart_flag:\n",
    "        print(\"Outlier removal in values of chart events ?\")\n",
    "        layout = widgets.Layout(width='100%', height='40px') #set width and height\n",
    "\n",
    "        radio_input5 = widgets.RadioButtons(options=['No outlier detection','Impute Outlier (default:98)','Remove outliers (default:98)'],value='No outlier detection',layout=layout)\n",
    "        display(radio_input5)\n",
    "        outlier=widgets.IntSlider(\n",
    "        value=98,\n",
    "        min=90,\n",
    "        max=99,\n",
    "        step=1,\n",
    "        disabled=False,layout={'width': '100%'}\n",
    "        )\n",
    "        left_outlier=widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=10,\n",
    "        step=1,\n",
    "        disabled=False,layout={'width': '100%'}\n",
    "        )\n",
    "        #display(oulier)\n",
    "        display(widgets.HBox([widgets.Label('Right Outlier Threshold',layout={'width': '150px'}), outlier]))\n",
    "        display(widgets.HBox([widgets.Label('Left Outlier Threshold',layout={'width': '150px'}), left_outlier]))\n",
    "    \n",
    "else:      \n",
    "    if lab_flag:\n",
    "        print(\"Outlier removal in values of lab events ?\")\n",
    "        layout = widgets.Layout(width='100%', height='40px') #set width and height\n",
    "\n",
    "        radio_input7 = widgets.RadioButtons(options=['No outlier detection','Impute Outlier (default:98)','Remove outliers (default:98)'],value='No outlier detection',layout=layout)\n",
    "        display(radio_input7)\n",
    "        outlier=widgets.IntSlider(\n",
    "        value=98,\n",
    "        min=90,\n",
    "        max=99,\n",
    "        step=1,\n",
    "        disabled=False,layout={'width': '100%'}\n",
    "        )\n",
    "        left_outlier=widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=10,\n",
    "        step=1,\n",
    "        disabled=False,layout={'width': '100%'}\n",
    "        )\n",
    "        #display(oulier)\n",
    "        display(widgets.HBox([widgets.Label('Right Outlier Threshold',layout={'width': '150px'}), outlier]))\n",
    "        display(widgets.HBox([widgets.Label('Left Outlier Threshold',layout={'width': '150px'}), left_outlier]))\n",
    "print(\"**Please run below cell to perform feature preprocessing**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "impossible-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh=0\n",
    "if data_icu:\n",
    "    if chart_flag:\n",
    "        clean_chart=radio_input5.value!='No outlier detection'\n",
    "        impute_outlier_chart=radio_input5.value=='Impute Outlier (default:98)'\n",
    "        thresh=outlier.value\n",
    "        left_thresh=left_outlier.value\n",
    "    preprocess_features_icu(cohort_output = cohort_output, \n",
    "                            save_path = target_path, \n",
    "                            diag_flag = False, \n",
    "                            group_diag = False,\n",
    "                            chart_flag = chart_flag,\n",
    "                            clean_chart = clean_chart,\n",
    "                            impute_outlier_chart = impute_outlier_chart,\n",
    "                            thresh = thresh,\n",
    "                            left_thresh = left_thresh)\n",
    "else:\n",
    "    # not adapted yet\n",
    "    print(\"Warning: this code hasn't been checked!\")\n",
    "    if lab_flag:\n",
    "        clean_lab=radio_input7.value!='No outlier detection'\n",
    "        impute_outlier=radio_input7.value=='Impute Outlier (default:98)'\n",
    "        thresh=outlier.value\n",
    "        left_thresh=left_outlier.value\n",
    "    preprocess_features_hosp(cohort_output, False,False,False,lab_flag,False,False,False,clean_lab,impute_outlier,thresh,left_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c4c60",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "Before encoding the data into time-series format, we can first load and visualize it to understand it better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ba669f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loading.load_preprocessed_data import DataLoader\n",
    "\n",
    "#data_loader = DataLoader(root_dir, cohort_output,data_mort,data_admn,data_los,diag_flag,proc_flag,out_flag,chart_flag,med_flag, lab_flag, micro_flag,impute, include,bucket,predW)\n",
    "data_loader = DataLoader(root_dir = root_dir, \n",
    "                         cohort_output = cohort_output,\n",
    "                         if_mort = data_mort,\n",
    "                         if_admn = data_admn,\n",
    "                         if_los = data_los,\n",
    "                         feat_cond = diag_flag,\n",
    "                         feat_proc = proc_flag,\n",
    "                         feat_out = out_flag,\n",
    "                         feat_chart = chart_flag,\n",
    "                         feat_med = med_flag, \n",
    "                         feat_lab = lab_flag, \n",
    "                         feat_micro = micro_flag,\n",
    "                         impute  = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "392d9364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ READ ADM FEATURES ]\n",
      "[ ======READING DIAGNOSIS ]\n",
      "[ ======READING PROCEDURES ]\n",
      "[ ======READING OUT EVENTS ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ======READING CHART EVENTS ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [01:16, 25.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ======READING MEDICATIONS ]\n",
      "[ ======READING LAB EVENTS ]\n",
      "[ READ ALL FEATURES ]\n",
      "Loaded the data for:  dict_keys(['data', 'cond', 'cond_per_adm', 'meds', 'proc', 'out', 'chart', 'lab'])\n"
     ]
    }
   ],
   "source": [
    "# load each table in a dictionary \n",
    "dataset = data_loader.load()\n",
    "print(\"Loaded the data for: \", dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a85bc436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2996,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can look at individual tables\n",
    "dataset['cond']['stay_id'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "053fd915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2662,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['proc']['stay_id'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-academy",
   "metadata": {},
   "source": [
    "## 7. Time-Series Representation\n",
    "In this section, please choose how you want to process and represent time-series data.\n",
    "\n",
    "- First option is to select the length of time-series data you want to include for this study. (Default is 72 hours)\n",
    "\n",
    "- Second option is to select bucket size which tells in what size time windows you want to divide your time-series.<br>\n",
    "For example, if you select **2** bucket size, it wil aggregate data for every 2 hours and <br>a time-series of length 24 hours will be represented as time-series with 12 time-windows <br>where data for every 2 hours is agggregated from original raw time-series.\n",
    "\n",
    "During this step, we will also save the time-series data in data dictionaries in the format that can be directly used for following deep learning analysis.\n",
    "\n",
    "### Imputation\n",
    "You can also choose if you want to impute lab/chart values. The imputation will be done by froward fill and mean or median imputation.<br>\n",
    "Values will be forward fill first and if no value exists for that admission we will use mean or median value for the patient.\n",
    "\n",
    "The data dictionaries will be saved in **./data/dict/**\n",
    "\n",
    "Please refer the readme to know the structure of data dictionaries.\n",
    "\n",
    "**Please run below cell to select time-series representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "mechanical-three",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Time-series Data Represenation=======\n",
      "Length of data to be included for time-series prediction ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of data_generation_icu failed: Traceback (most recent call last):\n",
      "  File \"/home/henrik/miniconda3/envs/mimic_prep_env/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/henrik/miniconda3/envs/mimic_prep_env/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/henrik/miniconda3/envs/mimic_prep_env/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/henrik/miniconda3/envs/mimic_prep_env/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 860, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 791, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"model/data_generation_icu.py\", line 656\n",
      "    t=t+1\n",
      "    ^\n",
      "IndentationError: unexpected indent\n",
      "]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e3871f4b544895bc3fbc01cc35c794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(options=('First 72 hours', 'First 48 hours', 'First 24 hours', 'Custom'), value='First 72 hours')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5bcfac97fd4f5bad52f9e66e6d4add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Fisrt (in hours):', layout=Layout(width='150px')), IntSlider(value=72, description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What time bucket size you want to choose ?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afef30c4b3f459598847a3fccd0b445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(options=('1 hour', '2 hour', '3 hour', '4 hour', '5 hour', 'Custom'), value='1 hour')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c48010cda34733b5868b3cecf6d8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Bucket Size (in hours):', layout=Layout(width='150px')), IntSlider(value=1, max=6,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to forward fill and mean or median impute lab/chart values to form continuous data signal?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d249e9dea3141a08c7fd5415b824891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(options=('No Imputation', 'forward fill and mean', 'forward fill and median'), value='No Imputati…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Please run below cell to perform time-series represenation and save in data dictionaries**\n"
     ]
    }
   ],
   "source": [
    "print(\"=======Time-series Data Represenation=======\")\n",
    "\n",
    "print(\"Length of data to be included for time-series prediction ?\")\n",
    "if(data_mort):\n",
    "    radio_input8 = widgets.RadioButtons(options=['First 72 hours','First 48 hours','First 24 hours','Custom'],value='First 72 hours')\n",
    "    display(radio_input8)\n",
    "    text2=widgets.IntSlider(\n",
    "    value=72,\n",
    "    min=24,\n",
    "    max=72,\n",
    "    step=1,\n",
    "    description='Fisrt',\n",
    "    disabled=False\n",
    "    )\n",
    "    display(widgets.HBox([widgets.Label('Fisrt (in hours):',layout={'width': '150px'}), text2]))\n",
    "elif(data_admn):\n",
    "    radio_input8 = widgets.RadioButtons(options=['Last 72 hours','Last 48 hours','Last 24 hours','Custom'],value='Last 72 hours')\n",
    "    display(radio_input8)\n",
    "    text2=widgets.IntSlider(\n",
    "    value=72,\n",
    "    min=24,\n",
    "    max=72,\n",
    "    step=1,\n",
    "    description='Last',\n",
    "    disabled=False\n",
    "    )\n",
    "    display(widgets.HBox([widgets.Label('Last (in hours):',layout={'width': '150px'}), text2]))\n",
    "elif(data_los):\n",
    "    radio_input8 = widgets.RadioButtons(options=['First 12 hours','First 24 hours','Custom'],value='First 24 hours')\n",
    "    display(radio_input8)\n",
    "    text2=widgets.IntSlider(\n",
    "    value=72,\n",
    "    min=12,\n",
    "    max=72,\n",
    "    step=1,\n",
    "    description='First',\n",
    "    disabled=False\n",
    "    )\n",
    "    display(widgets.HBox([widgets.Label('Fisrt (in hours):',layout={'width': '150px'}), text2]))\n",
    "else:\n",
    "    radio_input8 = widgets.RadioButtons(options=['First 72 hours','First 48 hours','First 24 hours','Custom'],value='First 72 hours')\n",
    "    display(radio_input8)\n",
    "    text2=widgets.IntSlider(\n",
    "    value=72,\n",
    "    min=24,\n",
    "    max=72,\n",
    "    step=1,\n",
    "    description='Fisrt',\n",
    "    disabled=False\n",
    "    )\n",
    "    display(widgets.HBox([widgets.Label('Fisrt (in hours):',layout={'width': '150px'}), text2]))\n",
    "    \n",
    "print(\"What time bucket size you want to choose ?\")\n",
    "radio_input7 = widgets.RadioButtons(options=['1 hour','2 hour','3 hour','4 hour','5 hour','Custom'],value='1 hour')\n",
    "display(radio_input7)\n",
    "text1=widgets.IntSlider(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=6,\n",
    "    step=1,\n",
    "    disabled=False\n",
    "    )\n",
    "#display(text1)\n",
    "display(widgets.HBox([widgets.Label('Bucket Size (in hours):',layout={'width': '150px'}), text1]))\n",
    "print(\"Do you want to forward fill and mean or median impute lab/chart values to form continuous data signal?\")\n",
    "radio_impute = widgets.RadioButtons(options=['No Imputation', 'forward fill and mean','forward fill and median'],value='No Imputation')\n",
    "display(radio_impute)   \n",
    "\n",
    "radio_input6 = widgets.RadioButtons(options=['0 hours','2 hours','4 hours','6 hours'],value='0 hours')\n",
    "if(data_mort):\n",
    "    print(\"If you have choosen mortality prediction task, then what prediction window length you want to keep?\")\n",
    "    radio_input6 = widgets.RadioButtons(options=['2 hours','4 hours','6 hours','8 hours','Custom'],value='2 hours')\n",
    "    display(radio_input6)\n",
    "    text3=widgets.IntSlider(\n",
    "    value=2,\n",
    "    min=2,\n",
    "    max=8,\n",
    "    step=1,\n",
    "    disabled=False\n",
    "    )\n",
    "    display(widgets.HBox([widgets.Label('Prediction window (in hours)',layout={'width': '180px'}), text3]))\n",
    "print(\"**Please run below cell to perform time-series represenation and save in data dictionaries**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "indie-appendix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ READ COHORT ]\n",
      "[ ======READING DIAGNOSIS ]\n",
      "[ ======READING PROCEDURES ]\n",
      "[ ======READING OUT EVENTS ]\n",
      "[ ======READING MEDICATIONS ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ READ ALL FEATURES ]\n",
      "include_time 72\n",
      "[ PROCESSED TIME SERIES TO EQUAL LENGTH  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 72/72 [00:01<00:00, 43.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ PROCESSED TIME SERIES TO EQUAL TIME INTERVAL ]\n",
      "> \u001b[0;32m/mnt/c/Users/HenrikvonKleist/OneDrive - Helmholtz Zentrum München/Dokumente/PhD/Code/Active Feature Acquisition/MIMIC-IV-Data-Pipeline-main/model/data_generation_icu.py\u001b[0m(706)\u001b[0;36msmooth_meds\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    704 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    705 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 706 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_meds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_proc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_chart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_lab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_micro\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    707 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    708 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> final_proc\n",
      "        stay_id  itemid  subject_id  start_time\n",
      "0      30008635  224275    15102490           0\n",
      "1      30008635  224277    15102490           0\n",
      "2      30008635  225792    15102490           0\n",
      "3      30014281  224276    17607166           0\n",
      "4      30014281  225469    17607166           0\n",
      "...         ...     ...         ...         ...\n",
      "17670  39288376  225459    17040759          71\n",
      "17671  39497668  226236    10022017          71\n",
      "17672  39522579  225402    11759515          71\n",
      "17673  39632317  225401    14950339          71\n",
      "17674  39874471  226236    14013548          71\n",
      "\n",
      "[17675 rows x 4 columns]\n",
      "ipdb> final_proc\n",
      "        stay_id  itemid  subject_id  start_time\n",
      "0      30008635  224275    15102490           0\n",
      "1      30008635  224277    15102490           0\n",
      "2      30008635  225792    15102490           0\n",
      "3      30014281  224276    17607166           0\n",
      "4      30014281  225469    17607166           0\n",
      "...         ...     ...         ...         ...\n",
      "17670  39288376  225459    17040759          71\n",
      "17671  39497668  226236    10022017          71\n",
      "17672  39522579  225402    11759515          71\n",
      "17673  39632317  225401    14950339          71\n",
      "17674  39874471  226236    14013548          71\n",
      "\n",
      "[17675 rows x 4 columns]\n",
      "ipdb> final_out\n",
      "        stay_id  itemid  value  start_time\n",
      "0      30008635  226559   50.0           0\n",
      "1      30014281  226633  150.0           0\n",
      "2      30030289  226559  320.0           0\n",
      "3      30031157  226559  125.0           0\n",
      "4      30031157  226575    0.0           0\n",
      "...         ...     ...    ...         ...\n",
      "92057  39931237  226559   75.0          71\n",
      "92058  39934109  226560  300.0          71\n",
      "92059  39938054  227510   35.0          71\n",
      "92060  39986786  226559  175.0          71\n",
      "92061  39997710  226559   80.0          71\n",
      "\n",
      "[92062 rows x 4 columns]\n",
      "ipdb> final_out.isna()\n",
      "       stay_id  itemid  value  start_time\n",
      "0        False   False  False       False\n",
      "1        False   False  False       False\n",
      "2        False   False  False       False\n",
      "3        False   False  False       False\n",
      "4        False   False  False       False\n",
      "...        ...     ...    ...         ...\n",
      "92057    False   False  False       False\n",
      "92058    False   False  False       False\n",
      "92059    False   False  False       False\n",
      "92060    False   False  False       False\n",
      "92061    False   False  False       False\n",
      "\n",
      "[92062 rows x 4 columns]\n",
      "ipdb> final_out.isna().sum()\n",
      "stay_id       0\n",
      "itemid        0\n",
      "value         0\n",
      "start_time    0\n",
      "dtype: int64\n",
      "ipdb> final_meds.isna().sum()\n",
      "stay_id       0\n",
      "itemid        0\n",
      "orderid       0\n",
      "stop_time     0\n",
      "subject_id    0\n",
      "rate          0\n",
      "amount        0\n",
      "start_time    0\n",
      "dtype: int64\n",
      "ipdb> exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7798/2450288376.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0minclude_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         predW=predW)\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mgen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_generation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcohort_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_mort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_admn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_los\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiag_flag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlab_flag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproc_flag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmed_flag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimpute\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/HenrikvonKleist/OneDrive - Helmholtz Zentrum München/Dokumente/PhD/Code/Active Feature Acquisition/MIMIC-IV-Data-Pipeline-main/model/data_generation_icu.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, cohort_output, if_mort, if_admn, if_los, feat_cond, feat_proc, feat_out, feat_chart, feat_med, feat_lab, feat_micro, impute, include_time, bucket, predW)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[ PROCESSED TIME SERIES TO EQUAL LENGTH  ]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_meds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[ SUCCESSFULLY SAVED DATA DICTIONARIES ]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/HenrikvonKleist/OneDrive - Helmholtz Zentrum München/Dokumente/PhD/Code/Active Feature Acquisition/MIMIC-IV-Data-Pipeline-main/model/data_generation_icu.py\u001b[0m in \u001b[0;36msmooth_meds\u001b[0;34m(self, bucket)\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_meds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_proc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_chart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_lab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_micro\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/HenrikvonKleist/OneDrive - Helmholtz Zentrum München/Dokumente/PhD/Code/Active Feature Acquisition/MIMIC-IV-Data-Pipeline-main/model/data_generation_icu.py\u001b[0m in \u001b[0;36msmooth_meds\u001b[0;34m(self, bucket)\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_meds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_proc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_chart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_lab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_micro\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mimic_prep_env/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mimic_prep_env/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if (radio_input6.value=='Custom'):\n",
    "    predW=int(text3.value)\n",
    "else:\n",
    "    predW=int(radio_input6.value[0].strip())\n",
    "if (radio_input7.value=='Custom'):\n",
    "    bucket=int(text1.value)\n",
    "else:\n",
    "    bucket=int(radio_input7.value[0].strip())\n",
    "if (radio_input8.value=='Custom'):\n",
    "    include=int(text2.value)\n",
    "else:\n",
    "    include=int(radio_input8.value.split()[1])\n",
    "if (radio_impute.value=='forward fill and mean'):\n",
    "    impute='Mean'\n",
    "elif (radio_impute.value=='forward fill and median'):\n",
    "    impute='Median'\n",
    "else:\n",
    "    impute=False\n",
    "\n",
    "if data_icu:\n",
    "    gen=data_generation_icu.Generator(\n",
    "        root_dir  = root_dir, \n",
    "        cohort_output = cohort_output,\n",
    "        if_mort = data_mort,\n",
    "        if_admn = data_admn,\n",
    "        # if_los = data_los,\n",
    "        if_los = True,\n",
    "        feat_cond = diag_flag,\n",
    "        feat_proc  = proc_flag,\n",
    "        feat_out = out_flag,\n",
    "        # feat_chart = chart_flag,\n",
    "        feat_chart = False,\n",
    "        feat_med = med_flag,\n",
    "        # feat_med = False,\n",
    "        # feat_lab = lab_flag,\n",
    "        feat_lab = False,\n",
    "        feat_micro =  micro_flag,\n",
    "        impute = impute,\n",
    "        include_time=include,\n",
    "        bucket=bucket,\n",
    "        predW=predW)\n",
    "else:\n",
    "    gen=data_generation.Generator(root_dir, cohort_output,data_mort,data_admn,data_los,diag_flag,lab_flag,proc_flag,med_flag,impute,include,bucket,predW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3221f",
   "metadata": {},
   "source": [
    "## Extract acute kidney injury information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea42f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: extract weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6da5a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_patient_weights(chart_df):\n",
    "    \"\"\"\n",
    "    Extract and compute average patient weights from the chart events DataFrame.\n",
    "\n",
    "    This function filters the input chart data for relevant weight item IDs, converts \n",
    "    measurements from pounds to kilograms where necessary, and calculates the average \n",
    "    weight for each patient (identified by `stay_id`). It also checks for large \n",
    "    discrepancies between different weight measurements and raises warnings if any \n",
    "    differences exceed 15 units.\n",
    "\n",
    "    Parameters:\n",
    "    - chart_df: DataFrame containing chart events. The DataFrame should have at least the following columns:\n",
    "        - stay_id: The unique identifier for the patient's ICU stay.\n",
    "        - itemid: The identifier for the type of measurement.\n",
    "        - value: The measurement value (e.g., weight).\n",
    "    \n",
    "    Returns:\n",
    "    - averages_df: DataFrame with the following columns:\n",
    "        - stay_id: The unique identifier for each patient's ICU stay.\n",
    "        - weight: The average weight for each patient, in kilograms.\n",
    "    \n",
    "    Notes:\n",
    "    - Converts weight values recorded in pounds (itemid 226531) to kilograms.\n",
    "    - Raises a warning if the difference between two weight measurements exceeds 15 kg.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the itemid_map for weight\n",
    "    itemid_map_weight = {\n",
    "        224639: {'name': 'Daily Weight', 'uom': 'kg'},\n",
    "        226512: {'name': 'Admission Weight (Kg)', 'uom': 'kg'},\n",
    "        226531: {'name': 'Admission Weight (lbs.)', 'uom': 'lbs'},\n",
    "    }\n",
    "    \n",
    "    # Define the itemids for weight\n",
    "    weight_itemids = list(itemid_map_weight.keys())\n",
    "    \n",
    "    # Filter the DataFrame for the relevant itemids\n",
    "    weights_df = chart_df[chart_df['itemid'].isin(weight_itemids)]\n",
    "    \n",
    "    # Convert the weight from pounds to kilograms for the last itemid\n",
    "    pound_to_kg_factor = 0.453592\n",
    "    weights_df.loc[weights_df['itemid'] == 226531, 'valuenum'] = (\n",
    "        weights_df.loc[weights_df['itemid'] == 226531, 'valuenum'] * pound_to_kg_factor )\n",
    "    \n",
    "    # Calculate the average weights for each stay_id\n",
    "    averages_df = weights_df.groupby('stay_id').agg({\n",
    "        'valuenum': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "\n",
    "    # Rename the 'value' column to indicate it's the average weight\n",
    "    averages_df.rename(columns={'valuenum': 'weight'}, inplace=True)\n",
    "    \n",
    "    # Check the differences between the weight columns\n",
    "    weight_means = weights_df.groupby('itemid')['valuenum'].mean()\n",
    "    \n",
    "    if len(weight_means) > 1:  # Ensure there are at least two itemid groups to compare\n",
    "        for i in range(len(weight_itemids) - 1):  # Loop through itemids\n",
    "            item1 = weight_itemids[i]\n",
    "            item2 = weight_itemids[i + 1]\n",
    "            if item1 in weight_means and item2 in weight_means:\n",
    "                diff = abs(weight_means[item1] - weight_means[item2])\n",
    "                if diff > 15:\n",
    "                    print(f\"Warning: Differences greater than 15 kg found between itemids {item1} and {item2}\")\n",
    "    \n",
    "    # Return only the average weight DataFrame\n",
    "    return averages_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f97b17bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df = extract_patient_weights(dataset['chart'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8fe5446b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2528,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_df['stay_id'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7faca8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stay_id</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30004242</td>\n",
       "      <td>76.766514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30008635</td>\n",
       "      <td>74.661243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30014281</td>\n",
       "      <td>95.992255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30020961</td>\n",
       "      <td>57.878339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30030100</td>\n",
       "      <td>92.714205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2523</th>\n",
       "      <td>39986935</td>\n",
       "      <td>67.857363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524</th>\n",
       "      <td>39993968</td>\n",
       "      <td>57.473936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>39994129</td>\n",
       "      <td>73.426584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2526</th>\n",
       "      <td>39995974</td>\n",
       "      <td>63.366802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2527</th>\n",
       "      <td>39997710</td>\n",
       "      <td>62.187463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2528 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stay_id     weight\n",
       "0     30004242  76.766514\n",
       "1     30008635  74.661243\n",
       "2     30014281  95.992255\n",
       "3     30020961  57.878339\n",
       "4     30030100  92.714205\n",
       "...        ...        ...\n",
       "2523  39986935  67.857363\n",
       "2524  39993968  57.473936\n",
       "2525  39994129  73.426584\n",
       "2526  39995974  63.366802\n",
       "2527  39997710  62.187463\n",
       "\n",
       "[2528 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: extract urine output information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f3b6fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_urine_output(out_df):\n",
    "    \"\"\"\n",
    "    Combine different versions of urine output into one version, summing measurements if there are conflicts\n",
    "    (i.e., multiple measurements at the same start_time) and generating warnings when combining measurements.\n",
    "\n",
    "    Parameters:\n",
    "    - out_df: DataFrame containing output events, including urine output.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with stay_id, urine_volume (summed), and start_time.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the itemid_map for urine output\n",
    "    itemid_map_urine = {\n",
    "        \"226627\": {'name': 'OR Urine', 'uom': 'mL'},\n",
    "        \"226631\": {'name': 'PACU Urine', 'uom': 'mL'},\n",
    "        # \"227488\": {'name': 'GU Irrigant Volume In', 'uom': 'mL'},\n",
    "        # \"227489\": {'name': 'GU Irrigant/Urine Volume Out', 'uom': 'mL'},\n",
    "        # \"227519\": {'name': 'Urine output_ApacheIV', 'uom': None},\n",
    "        # \"226566\": {'name': 'Urine and GU Irrigant Out', 'uom': 'mL'},\n",
    "        \"226559\": {'name': 'Foley', 'uom': 'mL'},\n",
    "        \"226561\": {'name': 'Condom Cath', 'uom': 'mL'},\n",
    "    }\n",
    "\n",
    "    # Filter out_df to only include the specified urine output itemids\n",
    "    itemids_urine = list(itemid_map_urine.keys())\n",
    "    filtered_df = out_df[out_df['itemid'].isin(map(int, itemids_urine))]\n",
    "\n",
    "    # Group by stay_id and start_time, then sum the 'value' if there are multiple entries\n",
    "    combined_df = filtered_df.groupby(['stay_id', 'start_time']).agg(\n",
    "        urine_volume=('value', 'sum')).reset_index()\n",
    "\n",
    "    # Check for conflicts (multiple rows combined)\n",
    "    conflicting_entries = filtered_df.groupby(['stay_id', 'start_time']).size()\n",
    "    conflicts = conflicting_entries[conflicting_entries > 1]\n",
    "    \n",
    "    if not conflicts.empty:\n",
    "        for index, num_measurements in conflicts.items():\n",
    "            stay_id, start_time = index\n",
    "            conflicting_values = filtered_df[(filtered_df['stay_id'] == stay_id) &\n",
    "                                             (filtered_df['start_time'] == start_time)]\n",
    "            conflicting_itemids = conflicting_values['itemid'].unique()\n",
    "            item_names = [itemid_map_urine[str(itemid)]['name'] for itemid in conflicting_itemids]\n",
    "\n",
    "            # print(f\"Warning: Combining {num_measurements} measurements at stay_id {stay_id}, start_time {start_time}.\")\n",
    "            # print(f\"Itemids combined: {conflicting_itemids}, names: {item_names}\")\n",
    "            # print(f\"Urine volumes combined: {conflicting_values['value'].tolist()}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b4b430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_df  = combine_urine_output(out_df = dataset['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1cdb36b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stay_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>urine_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30004242</td>\n",
       "      <td>14</td>\n",
       "      <td>925.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30004242</td>\n",
       "      <td>18</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30004242</td>\n",
       "      <td>19</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30004242</td>\n",
       "      <td>21</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30004242</td>\n",
       "      <td>22</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107766</th>\n",
       "      <td>39997710</td>\n",
       "      <td>321</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107767</th>\n",
       "      <td>39997710</td>\n",
       "      <td>326</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107768</th>\n",
       "      <td>39997710</td>\n",
       "      <td>328</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107769</th>\n",
       "      <td>39997710</td>\n",
       "      <td>330</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107770</th>\n",
       "      <td>39997710</td>\n",
       "      <td>332</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107771 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         stay_id  start_time  urine_volume\n",
       "0       30004242          14         925.0\n",
       "1       30004242          18          75.0\n",
       "2       30004242          19         175.0\n",
       "3       30004242          21          75.0\n",
       "4       30004242          22         175.0\n",
       "...          ...         ...           ...\n",
       "107766  39997710         321         250.0\n",
       "107767  39997710         326         160.0\n",
       "107768  39997710         328         100.0\n",
       "107769  39997710         330          90.0\n",
       "107770  39997710         332          90.0\n",
       "\n",
       "[107771 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e043215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize urine output by weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dba3f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def normalize_urine_output(urine_df: pd.DataFrame, weight_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalizes urine output to urine volume per kilogram of weight. Imputes missing weights with the mean and provides a warning about the imputation.\n",
    "\n",
    "    Parameters:\n",
    "    - urine_df (pd.DataFrame): Dataframe containing urine output data with columns ['stay_id', 'start_time', 'urine_volume'].\n",
    "    - weight_df (pd.DataFrame): Dataframe containing weight data with columns ['stay_id', 'weight'].\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A dataframe containing ['stay_id', 'start_time', 'urine_volume_per_kg'] after normalization.\n",
    "    \n",
    "    Note:\n",
    "    - If a weight is missing for a stay_id, the function imputes the missing weight using the mean of the weights from the `weight_df`.\n",
    "    - A warning is displayed with the percentage of weights that were imputed.\n",
    "    \"\"\"\n",
    "    # Merge urine_df with weight_df on 'stay_id'\n",
    "    merged_df = pd.merge(urine_df, weight_df, on='stay_id', how='left')\n",
    "    \n",
    "    # Calculate mean weight for imputation\n",
    "    mean_weight = merged_df['weight'].mean()\n",
    "\n",
    "    # Count missing weights\n",
    "    missing_weights_count = merged_df['weight'].isna().sum()\n",
    "\n",
    "    # Impute missing weights with the mean weight\n",
    "    merged_df['weight'].fillna(mean_weight, inplace=True)\n",
    "\n",
    "    # Issue a warning about the percentage of missing weights imputed\n",
    "    if missing_weights_count > 0:\n",
    "        imputed_percentage = (missing_weights_count / len(merged_df)) * 100\n",
    "        print(f\"Warning: {imputed_percentage:.2f}% of the weights were imputed using the mean value.\")\n",
    "\n",
    "    # Normalize urine volume per kg\n",
    "    merged_df['urine_volume_per_kg'] = merged_df['urine_volume'] / merged_df['weight']\n",
    "\n",
    "    # Return the new dataframe with the required columns\n",
    "    return merged_df[['stay_id', 'start_time', 'urine_volume_per_kg']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "acd4d5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 4.19% of the weights were imputed using the mean value.\n"
     ]
    }
   ],
   "source": [
    "urine_norm_df = normalize_urine_output(urine_df = urine_df, weight_df = weight_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82504018",
   "metadata": {},
   "source": [
    "## Visualize the dynamic data\n",
    "Now, since we processed the data into a temporal format, we can now visualize the resulting tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "486e78a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:   7%|████▊                                                           | 224/2999 [02:08<38:44,  1.19id/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping id: 32029950 (missing file)\n",
      "Skipping id: 33957882 (missing file)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  38%|███████████████████████                                      | 1133/2999 [22:10<1:27:17,  2.81s/id]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping id: 35065251 (missing file)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  60%|█████████████████████████████████████▍                         | 1785/2999 [53:25<36:20,  1.80s/id]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7798/3268306760.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                  \u001b[0mnum_stays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtemporal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/c/Users/HenrikvonKleist/OneDrive - Helmholtz Zentrum München/Dokumente/PhD/Code/Active Feature Acquisition/MIMIC-IV-Data-Pipeline-main/data_loading/load_preprocessed_dynamic_data.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# Load dynamic, static, and demographic data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mdyn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mdemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mimic_prep_env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mimic_prep_env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mimic_prep_env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0mnew_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_currow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mimic_prep_env/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    433\u001b[0m             )\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mimic_prep_env/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         ]\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mimic_prep_env/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# from BlockManager perspective\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mimic_prep_env/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             val = sanitize_array(\n\u001b[0;32m--> 323\u001b[0;31m                 \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m             )\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from data_loading.load_preprocessed_dynamic_data import DynamicDataLoader\n",
    "\n",
    "# Load data with default paths\n",
    "data_loader = DynamicDataLoader( data_icu=True, \n",
    "                                 root_dir = root_dir,\n",
    "                                cohort_output = cohort_output,\n",
    "                                 data_dir='./data/csv/', \n",
    "                                 num_stays=None\n",
    "                               )\n",
    "temporal_df, static_df = data_loader.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2eadcc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([( 'MEDS', '222168'),\n",
       "            ( 'MEDS', '225158'),\n",
       "            ( 'MEDS', '225943'),\n",
       "            ( 'MEDS', '221906'),\n",
       "            ( 'MEDS', '225823'),\n",
       "            ( 'MEDS', '225942'),\n",
       "            ( 'MEDS', '222042'),\n",
       "            ( 'MEDS', '221668'),\n",
       "            ( 'MEDS', '225828'),\n",
       "            ( 'MEDS', '220949'),\n",
       "            ...\n",
       "            ('CHART', '227607'),\n",
       "            ('CHART', '227608'),\n",
       "            ('CHART', '227609'),\n",
       "            ('CHART', '227612'),\n",
       "            ('CHART', '227613'),\n",
       "            ('CHART', '227618'),\n",
       "            ('CHART', '227596'),\n",
       "            ('CHART', '229993'),\n",
       "            (   'id',       ''),\n",
       "            ( 'time',       '')],\n",
       "           length=2986)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2b834ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read items\n",
    "df_items = pd.read_csv(root_dir + '/mimiciv/3.0/icu/d_items.csv.gz')\n",
    "\n",
    "def rename_columns_with_descriptions(df_items, df_data):\n",
    "    # Create a dictionary to map item_id to description\n",
    "    id_to_description = df_items.set_index('itemid')['label'].to_dict()\n",
    "\n",
    "    # Count the number of unique first-level index values and how many columns belong to each\n",
    "    first_index_counts = df_data.columns.get_level_values(0).value_counts()\n",
    "\n",
    "    # Print the results\n",
    "    for first_index, count in first_index_counts.items():\n",
    "        print(f\"Category: {first_index} has {count} different features\")\n",
    "    \n",
    "    # Create a new list of column names\n",
    "    new_columns = []\n",
    "    for col in df_data.columns:\n",
    "        if isinstance(col, tuple):\n",
    "            # Multi-level column\n",
    "            first_level = col[0]  # e.g., 'MEDS', 'CHART'\n",
    "            second_level = col[1]  # e.g., item_id\n",
    "            \n",
    "            # Get the description for the second level; use the item_id if no description found\n",
    "            if len(second_level) > 0:\n",
    "                label = id_to_description.get(int(second_level), \"\")\n",
    "\n",
    "                # Combine first level column name and description\n",
    "                new_col_name = f\"{first_level}_{label}\" if label else first_level\n",
    "            else:\n",
    "                new_col_name = f\"{first_level}\"\n",
    "        else:\n",
    "            # Single-level column; keep it unchanged\n",
    "            new_col_name = col\n",
    "        \n",
    "        # Avoid adding an underscore for single-level columns like 'id' and 'time'\n",
    "        if new_col_name.endswith('_'):\n",
    "            new_col_name = new_col_name[:-1]  # Remove trailing underscore if present\n",
    "\n",
    "        new_columns.append(new_col_name)\n",
    "    \n",
    "    # Rename the columns with the new names\n",
    "    df_data.columns = new_columns\n",
    "    \n",
    "    return df_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dafb6910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: CHART has 1995 different features\n",
      "Category: LABS has 657 different features\n",
      "Category: PROC has 138 different features\n",
      "Category: MEDS has 128 different features\n",
      "Category: OUT has 66 different features\n",
      "Category: time has 1 different features\n",
      "Category: id has 1 different features\n"
     ]
    }
   ],
   "source": [
    "df_data = rename_columns_with_descriptions(df_items = df_items, df_data = temporal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae525217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8456b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_df['stay_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfd7610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa91777",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf15a2b",
   "metadata": {},
   "source": [
    "## Add AKI label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids_in_urine = urine_df['stay_id'].nunique()\n",
    "print(unique_ids_in_urine )\n",
    "unique_ids_in_data = df_data ['id'].nunique()\n",
    "print(unique_ids_in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f2ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeee58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_urine_output_and_aki_label(urine_df, df_temporal):\n",
    "    # Merge urine output per hour from df1 into df2 based on stay_id and start_time\n",
    "    df_temporal = df_temporal.merge(urine_df[['stay_id', 'start_time', 'urine_volume_per_kg']], \n",
    "                    left_on=['id', 'time'], \n",
    "                    right_on=['stay_id', 'start_time'], \n",
    "                    how='left')\n",
    "    \n",
    "    # Fill missing urine output values with 0\n",
    "    df_temporal['urine_volume_per_kg'] = df_temporal['urine_volume_per_kg'].fillna(0)\n",
    "    \n",
    "    # Calculate 12-hour rolling sum and average urine output over the last 12 hours\n",
    "    df_temporal['urine_volume_per_kg_12hr_avg'] = df_temporal.groupby('id')['urine_volume_per_kg']\\\n",
    "                                        .transform(lambda x: x.rolling(window=12, min_periods=1).sum() / 12)\n",
    "    \n",
    "    # Add AKI label based on the condition\n",
    "    df_temporal['aki_label'] = (df_temporal['urine_volume_per_kg_12hr_avg'] < 0.5).astype(int)\n",
    "\n",
    "    \n",
    "    # compute fraction of positive cases: \n",
    "    # Step 1: Identify stay_ids with at least one aki_label == 1\n",
    "    stay_ids_with_aki = df_temporal[df_temporal['aki_label'] == 1]['stay_id'].unique()\n",
    "\n",
    "    # Step 2: Count the total number of unique stay_ids\n",
    "    total_stay_ids = df_temporal['id'].nunique()\n",
    "\n",
    "    # Step 3: Compute the fraction\n",
    "    fraction_with_aki = len(stay_ids_with_aki) / total_stay_ids\n",
    "\n",
    "    # Output the result\n",
    "    print(f\"Fraction of stay_ids with at least one aki_label == 1: {fraction_with_aki:.2f}\")\n",
    "\n",
    "    return df_temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e24c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temporal = add_urine_output_and_aki_label(urine_df = urine_norm_df, df_temporal = df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765541eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "urine_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b9b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can you compute \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776638ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7e6afc6",
   "metadata": {},
   "source": [
    "## Visualize the dynamic data\n",
    "Now, since we processed the data into a temporal format, we can now visualize the resulting tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=======Machine :earning Models=======\")\n",
    "radio_input5 = widgets.RadioButtons(options=['Logistic Regression','Random Forest','Gradient Bossting','Xgboost'],value='Gradient Bossting')\n",
    "display(radio_input5)\n",
    "print(\"Do you wnat to conactenate the time-series feature\")\n",
    "radio_input6 = widgets.RadioButtons(options=['Conactenate','Aggregate'],value='Conactenate')\n",
    "display(radio_input6)\n",
    "print(\"Please select below option for cross-validation\")\n",
    "radio_input7 = widgets.RadioButtons(options=['No CV','5-fold CV','10-fold CV'],value='5-fold CV')\n",
    "display(radio_input7)\n",
    "print(\"Do you want to do oversampling for minority calss ?\")\n",
    "radio_input8 = widgets.RadioButtons(options=['True','False'],value='True')\n",
    "display(radio_input8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-miller",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if radio_input7.value=='No CV':\n",
    "    cv=0\n",
    "elif radio_input7.value=='5-fold CV':\n",
    "    cv=int(5)\n",
    "elif radio_input7.value=='10-fold CV':\n",
    "    cv=int(10)\n",
    "ml=ml_models.ML_models(data_icu,cv,radio_input5.value,concat=radio_input6.value=='Conactenate',oversampling=radio_input8.value=='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-chancellor",
   "metadata": {},
   "source": [
    "## 9. Deep Learning Models\n",
    "- Time-series LSTM and Time-series CNN which will only use time-series events like medications, charts, labs, output events to train model.\n",
    "\n",
    "- Hybrid LSTM and Hybrid CNN will use static data - diagnosis, demographic data aong with other time-series data to train model.\n",
    "\n",
    "- LSTM with Attention model will use attention layer to rank the important features and learn to predict output. It will use both static and time-series data.\n",
    "\n",
    "**Go to ./model/parameter.py and define all variables needed for model building and training**\n",
    "\n",
    "**Please run below cell to select which model to use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_input6=widgets.RadioButtons(options=['Time-series LSTM','Time-series CNN','Hybrid LSTM','Hybrid CNN'],value='Time-series LSTM')\n",
    "display(radio_input6)\n",
    "print(\"Please select below option for cross-validation\")\n",
    "radio_input7 = widgets.RadioButtons(options=['No CV','5-fold CV','10-fold CV'],value='5-fold CV')\n",
    "display(radio_input7)\n",
    "print(\"Do you want to do oversampling for minority calss ?\")\n",
    "radio_input8 = widgets.RadioButtons(options=['True','False'],value='True')\n",
    "display(radio_input8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-stewart",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if radio_input7.value=='No CV':\n",
    "    cv=0\n",
    "elif radio_input7.value=='5-fold CV':\n",
    "    cv=int(5)\n",
    "elif radio_input7.value=='10-fold CV':\n",
    "    cv=int(10)\n",
    "    \n",
    "if data_icu:\n",
    "    model=dl_train.DL_models(data_icu,diag_flag,proc_flag,out_flag,chart_flag,med_flag,False,radio_input6.value,cv,oversampling=radio_input8.value=='True',model_name='attn_icu_read',train=True)\n",
    "else:\n",
    "    model=dl_train.DL_models(data_icu,diag_flag,proc_flag,False,False,med_flag,lab_flag,radio_input6.value,cv,oversampling=radio_input8.value=='True',model_name='attn_icu_read',train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-factor",
   "metadata": {},
   "source": [
    "## 10. Running BEHRT\n",
    "Below we integrate the implementation of BEHRT in our pipeline.\n",
    "We perform pre-procesing needed to run BEHRT model. https://github.com/deepmedicine/BEHRT\n",
    "\n",
    "Few things to note before running BEHRT -\n",
    "- The numerical values are binned into quantiles.\n",
    "- BEHRT has recommended maximum number of events per sample to be 512. \n",
    "    So feature selection is important so that number of events per sample does not exceed 512.\n",
    "- The model is quite computationally heavy so it requires a GPU.\n",
    "\n",
    "The output files for BEHRT will be saved in ./data/behrt/ folder\n",
    "\n",
    "**Please run below cell to to pre-process and run BEHRT on the selected cohort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_icu:\n",
    "    token=tokenization.BEHRT_models(data_icu,diag_flag,proc_flag,out_flag,chart_flag,med_flag,False)\n",
    "    tokenized_src, tokenized_age, tokenized_gender, tokenized_ethni, tokenized_ins, tokenized_labels=token.tokenize()\n",
    "else:\n",
    "    token=tokenization.BEHRT_models(data_icu,diag_flag,proc_flag,False,False,med_flag,lab_flag)\n",
    "    tokenized_src, tokenized_age, tokenized_gender, tokenized_ethni, tokenized_ins, tokenized_labels=token.tokenize()\n",
    "    \n",
    "behrt_train.train_behrt(tokenized_src, tokenized_age, tokenized_gender, tokenized_ethni, tokenized_ins, tokenized_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-objective",
   "metadata": {},
   "source": [
    "### EVALUATION AS STANDALONE MODULE\n",
    "Below cell shows an exaple of how evaluation module can be used as a standalone module.\n",
    "\n",
    "evaluation.Loss class can be instantiated and model output and ground truth can be passed to it to obtain results.\n",
    "\n",
    "In the example below we captured model output and ground truth in a file and used that file to read the data.\n",
    "\n",
    "In function definition ***loss(prob,truth,logits,False)***\n",
    "\n",
    "prob -> List of Output predicted probabilities of case being positive\n",
    "\n",
    "truth -> List of ground truth labels\n",
    "\n",
    "logits -> List of logits obtained from last fully connected layer before applying softmax.sigmoid function in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device='cuda:0'\n",
    "#device='cpu'\n",
    "loss=evaluation.Loss(device,acc=True,ppv=True,sensi=True,tnr=True,npv=True,auroc=True,aurocPlot=True,auprc=True,auprcPlot=True,callb=True,callbPlot=True)\n",
    "with open(\"./data/output/outputDict\", 'rb') as fp:\n",
    "    outputDict=pickle.load(fp)\n",
    "prob=list(outputDict['Prob'])\n",
    "truth=list(outputDict['Labels'])\n",
    "logits=list(outputDict['Logits'])\n",
    "#print(torch.tensor(prob))\n",
    "print(\"======= TESTING ========\")\n",
    "loss(prob,truth,logits,train=False,standalone=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-works",
   "metadata": {},
   "source": [
    "### 11. FAIRNESS EVALUATION\n",
    "In train and testing step we save output files in **./data/output/** folder.\n",
    "\n",
    "This file conatins list of demographic variables included in training and testing of the model.\n",
    "\n",
    "It also contains the ground truth labels and predicted probability for each sample.\n",
    "\n",
    "We use the above saved data to perform fairness evaluation of the results obtained from model testing.\n",
    "\n",
    "This module can be used as stand-alone module also.\n",
    "\n",
    "Please create a file that contains predicted probabilites form the last sigmoid layer in column named **Prob** and\n",
    "ground truth labels for each sample in column named **Labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness.fairness_evaluation(inputFile='outputDict',outputFile='fairnessReport')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-distinction",
   "metadata": {},
   "source": [
    "### 12. MODEL CALLIBRATION\n",
    "\n",
    "Please run below cell if you want to callibrate predicted probabilites of the model on test data.\n",
    "It will use the output saved during the testing of the model.\n",
    "\n",
    "The file is saved in **./data/output/**.\n",
    "\n",
    "This module can be used as stand-alone module also.\n",
    "\n",
    "Please create a file that contain predicted logits form the last fully connected layer in column named **Logits** and <br>ground truth labels for each sample in a column named **Labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "callibrate_output.callibrate(inputFile='outputDict',outputFile='callibratedResults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-canyon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (mimic_prep_env)",
   "language": "python",
   "name": "mimic_prep_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
